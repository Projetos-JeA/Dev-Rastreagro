# üß† Guia de Instala√ß√£o e Configura√ß√£o do Ollama

## O que √© Ollama?

Ollama √© uma ferramenta **gratuita e local** para rodar modelos de IA grandes (LLMs) no seu computador. N√£o precisa de internet e n√£o tem custos.

## üöÄ Instala√ß√£o

### Windows

1. Baixe o instalador: https://ollama.com/download
2. Execute o instalador
3. Ollama ser√° instalado e iniciado automaticamente

### Verificar Instala√ß√£o

Abra o PowerShell e execute:
```powershell
ollama --version
```

Se aparecer a vers√£o, est√° instalado! ‚úÖ

## üì¶ Instalar Modelos

O sistema usa dois modelos:

### 1. Modelo de Embeddings (obrigat√≥rio)
```bash
ollama pull nomic-embed-text
```

### 2. Modelo Principal (opcional, para an√°lises complexas)
```bash
ollama pull llama3.2
```

## üîß Configura√ß√£o no Projeto

### 1. Instalar biblioteca Python

```bash
cd backend
pip install ollama numpy
```

### 2. Verificar se Ollama est√° rodando

Ollama precisa estar rodando em segundo plano. Verifique:

```bash
ollama list
```

Se retornar lista de modelos, est√° funcionando! ‚úÖ

## üß™ Testando

### Teste Manual

```python
import ollama

# Testar embeddings
response = ollama.embeddings(model="nomic-embed-text", prompt="teste")
print(response["embedding"][:5])  # Primeiros 5 valores
```

### Teste no Sistema

1. Inicie o backend
2. Acesse `/quotations/relevant` (endpoint usa IA automaticamente)
3. Verifique os logs do backend

## ‚ö†Ô∏è Problemas Comuns

### "Ollama n√£o est√° instalado"
- Instale o Ollama: https://ollama.com/download
- Reinicie o terminal

### "Connection refused"
- Ollama n√£o est√° rodando
- Inicie o Ollama manualmente ou reinicie o computador

### "Model not found"
- Execute: `ollama pull nomic-embed-text`
- Execute: `ollama pull llama3.2`

### Sistema lento
- Ollama consome recursos do computador
- Feche outros programas pesados
- Use modelos menores se necess√°rio

## üí° Fallback Autom√°tico

Se o Ollama n√£o estiver dispon√≠vel, o sistema usa um **fallback simples** baseado em:
- Palavras-chave
- Categorias
- Localiza√ß√£o

Funciona, mas com menor precis√£o que a IA completa.

## üìä Performance

- **Com Ollama**: Matching inteligente com embeddings
- **Sem Ollama**: Matching baseado em regras simples

Ambos funcionam, mas Ollama oferece melhor precis√£o!

## üîó Links √öteis

- **Site Oficial**: https://ollama.com
- **Documenta√ß√£o**: https://github.com/ollama/ollama
- **Modelos Dispon√≠veis**: https://ollama.com/library

---

**√öltima atualiza√ß√£o**: 2025-11-29


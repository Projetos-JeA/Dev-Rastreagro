# ğŸš€ InstalaÃ§Ã£o do Ollama - Passo a Passo

## âœ… Passo 1: Verificar InstalaÃ§Ã£o

ApÃ³s instalar o Ollama, **feche e reabra o PowerShell/Terminal**.

Depois, teste:

```powershell
ollama --version
```

Se aparecer a versÃ£o, estÃ¡ funcionando! âœ…

---

## ğŸ“¦ Passo 2: Baixar Modelos NecessÃ¡rios

### Modelo OBRIGATÃ“RIO (Embeddings)

Este Ã© o modelo mais importante para o sistema funcionar:

```powershell
ollama pull nomic-embed-text
```

**Tamanho**: ~274 MB  
**Tempo**: 1-2 minutos (dependendo da internet)

### Modelo OPCIONAL (AnÃ¡lises)

Para anÃ¡lises mais complexas (pode usar fallback simples):

```powershell
ollama pull llama3.2
```

**Tamanho**: ~2.0 GB  
**Tempo**: 5-10 minutos (dependendo da internet)

---

## ğŸ” Passo 3: Verificar Modelos Instalados

```powershell
ollama list
```

VocÃª deve ver algo como:

```
NAME                  ID              SIZE    MODIFIED
nomic-embed-text:latest  abc123...      274 MB  2 hours ago
llama3.2:latest         def456...      2.0 GB  1 hour ago
```

---

## âš ï¸ IMPORTANTE: Modelos Cloud vs Locais

### âŒ Modelos Cloud (NÃƒO usar)

- `gpt-oss:20b-cloud`
- `deepseek-v3.1:671b-cloud`
- `qwen3-coder:480b-cloud`
- `minimax-m2:cloud`
- `glm-4.6:cloud`

**Por quÃª?** Esses modelos sÃ£o cloud-based e podem ter custos ou limitaÃ§Ãµes.

### âœ… Modelos Locais (USAR)

- `nomic-embed-text` â† **OBRIGATÃ“RIO**
- `llama3.2` â† Recomendado
- `llama3.1`
- `mistral`
- `phi3`

**Por quÃª?** Rodam localmente, sem custos, sem internet.

---

## ğŸ§ª Passo 4: Testar Ollama

### Teste 1: Verificar se estÃ¡ rodando

```powershell
ollama list
```

### Teste 2: Testar embeddings

```powershell
ollama run nomic-embed-text "teste"
```

### Teste 3: Testar modelo principal (se instalou)

```powershell
ollama run llama3.2 "OlÃ¡, como vocÃª estÃ¡?"
```

---

## ğŸ”§ Passo 5: Instalar DependÃªncias Python

```powershell
cd backend
pip install ollama numpy
```

---

## ğŸ—„ï¸ Passo 6: Rodar Migration

```powershell
cd backend
alembic upgrade head
```

Isso cria a tabela `user_interactions` no banco.

---

## âœ… Passo 7: Testar o Sistema

1. Inicie o backend:

```powershell
cd backend
python -m uvicorn main:app --reload
```

2. Teste o endpoint:

```powershell
# No navegador ou Postman
GET http://localhost:8000/quotations/relevant
```

3. Verifique os logs do backend para ver se a IA estÃ¡ funcionando.

---

## ğŸ› Problemas Comuns

### "ollama: comando nÃ£o encontrado"

- **SoluÃ§Ã£o**: Feche e reabra o terminal
- Se nÃ£o funcionar, reinicie o computador

### "Connection refused"

- **SoluÃ§Ã£o**: Ollama precisa estar rodando
- Verifique se o serviÃ§o Ollama estÃ¡ ativo
- Reinicie o Ollama se necessÃ¡rio

### "Model not found"

- **SoluÃ§Ã£o**: Execute `ollama pull nomic-embed-text`
- Aguarde o download completar

### Sistema lento

- **Normal**: Ollama consome recursos do computador
- Feche outros programas pesados
- Use modelos menores se necessÃ¡rio

---

## ğŸ“Š Resumo dos Modelos

| Modelo             | Tamanho | ObrigatÃ³rio? | Uso                           |
| ------------------ | ------- | ------------ | ----------------------------- |
| `nomic-embed-text` | 274 MB  | âœ… SIM       | Embeddings para matching      |
| `llama3.2`         | 2.0 GB  | âŒ NÃ£o       | AnÃ¡lises complexas (opcional) |

---

## ğŸ¯ PrÃ³ximos Passos

1. âœ… Instalar Ollama
2. âœ… Baixar `nomic-embed-text`
3. â³ Instalar dependÃªncias Python
4. â³ Rodar migration
5. â³ Testar sistema

---

**Ãšltima atualizaÃ§Ã£o**: 2025-11-29
